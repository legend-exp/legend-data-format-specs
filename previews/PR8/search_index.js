var documenterSearchIndex = {"docs":
[{"location":"daq_data/#Low-level-DAQ-Data-Structure","page":"DAQ Data","title":"Low-level DAQ Data Structure","text":"","category":"section"},{"location":"daq_data/","page":"DAQ Data","title":"DAQ Data","text":"DAQ data is represented by a table, each row represents a DAQ event on a single (physical or logical) input channel. Event building will happen at a higher data level. The detailed structure of DAQ data will depend on the DAQ system and experimental setup.","category":"page"},{"location":"daq_data/#Waveform-vectors","page":"DAQ Data","title":"Waveform vectors","text":"","category":"section"},{"location":"daq_data/","page":"DAQ Data","title":"DAQ Data","text":"Waveform vectors are regular Tables with three columns (table{t0,dt,values}):","category":"page"},{"location":"daq_data/","page":"DAQ Data","title":"DAQ Data","text":"t0: the waveform time offsets (ralative to a certain global reference), optionally with units\ndt: the waveform sampling periods, optionally with units\nvalues: the waveform values. May be Array of equal-sized arrays, Vector of vectors, etc. t0 and dt must have one dimension (the last) less than values.","category":"page"},{"location":"daq_data/","page":"DAQ Data","title":"DAQ Data","text":"Example:","category":"page"},{"location":"daq_data/","page":"DAQ Data","title":"DAQ Data","text":"GROUP \"waveform\" {\n    ATTRIBUTE \"datatype\" = \"table{t0,dt,values}\"\n    DATASET \"dt\" {\n        ATTRIBUTE \"datatype\" = \"array<1>{real}\"\n        ATTRIBUTE \"units\" = \"ns\"\n        DATA = [10, 10, 10, ...]\n    }\n    DATASET \"t0\" {\n        ATTRIBUTE \"datatype\"= \"array<1>{real}\"\n        ATTRIBUTE \"units\" = \"ns\"\n        DATA = [76420, 76420, 76420, ...]\n    }\n    GROUP \"values\" {\n        ATTRIBUTE \"datatype\"= \"array<1>{array<1>{real}}\"\n        DATASET \"cumulative_length\" {\n            ATTRIBUTE \"datatype\" = \"array<1>{real}\"\n            DATA = [1000, 2000, 3000, 4000, ...]\n        }\n        DATASET \"flattened_data\" {\n            ATTRIBUTE \"datatype\" = \"array<1>{real}\"\n            DATA = [14440, 14442, 14441, 14434, ...]\n        }\n    }\n}","category":"page"},{"location":"daq_data/#Generic-DAQ-data-example","page":"DAQ Data","title":"Generic DAQ data example","text":"","category":"section"},{"location":"daq_data/","page":"DAQ Data","title":"DAQ Data","text":"A table daqdata with columns for channel number, unix-time, event type, veto and waveform will be written to an HDF5 file like this:","category":"page"},{"location":"daq_data/","page":"DAQ Data","title":"DAQ Data","text":"GROUP \"daqdata\" {\n    ATTRIBUTE \"datatype\" = \"table{ch,unixtime,evttype,veto,waveform}\"\n    DATASET \"ch\" {\n        ATTRIBUTE \"datatype\" = \"array<1>{real}\"\n        DATA = [1, 3, 2, 4, ...]\n    }\n    DATASET \"unixtime\" {\n        ATTRIBUTE \"datatype\" = \"array<1>{real}\"\n        DATA = [1.44061e+09, 1.44061e+09, ...]\n    }\n    DATASET \"evttype\" {\n        ATTRIBUTE \"datatype\" = \"array<1>{enum{evt_undef=0,evt_real=1,evt_pulser=2,evt_mc=3,evt_baseline=4}}\"\n        DATA = [1, 2, 1, 1, ...]\n    }\n    DATASET \"veto\" {\n        DATA = [1, 1, 0, 0, ...]\n        ATTRIBUTE \"datatype\" = \"array<1>{bool}\"\n        DATA = [1, 1, 0, 0, ...]\n    }\n    GROUP \"waveform\" {\n        ATTRIBUTE \"datatype\" = \"table{t0,dt,values}\"\n        DATASET \"dt\" {\n            ATTRIBUTE \"datatype\"= \"array<1>{real}\"\n            ATTRIBUTE \"units\"= \"ns\"\n            DATA = [10, 10, 10, ...]\n        }\n        DATASET \"t0\" {\n            ATTRIBUTE \"datatype\"= \"array<1>{real}\"\n            ATTRIBUTE \"units\"= \"ns\"\n            DATA = [76420, 76420, 76420, ...]\n        }\n        GROUP \"values\" {\n            ATTRIBUTE \"datatype\"= \"array<1>{array<1>{real}}\"\n            DATASET \"cumulative_length\" {\n                ATTRIBUTE \"datatype\" = \"array<1>{real}\"\n                DATA = [1000, 2000, 3000, 4000, ...]\n            }\n            DATASET \"flattened_data\" {\n                ATTRIBUTE \"datatype\" = \"array<1>{real}\"\n                DATA = [14440, 14442, 14441, 14434, ...]\n            }\n        }\n    }\n}","category":"page"},{"location":"daq_data/","page":"DAQ Data","title":"DAQ Data","text":"The actual numeric types of the datasets will be application-dependent.","category":"page"},{"location":"daq_data/#LEGEND-FlashCam-DAQ-data-read-out-by-Orca","page":"DAQ Data","title":"LEGEND FlashCam DAQ data read out by Orca","text":"","category":"section"},{"location":"daq_data/","page":"DAQ Data","title":"DAQ Data","text":"Full format specification of the raw tier data.","category":"page"},{"location":"daq_data/","page":"DAQ Data","title":"DAQ Data","text":"Coming Soon...","category":"page"},{"location":"hdf5/#HDF5-File-Format","page":"HDF5","title":"HDF5 File Format","text":"","category":"section"},{"location":"hdf5/","page":"HDF5","title":"HDF5","text":"Pages = [\"hdf5.md\"]\nDepth = 5","category":"page"},{"location":"hdf5/","page":"HDF5","title":"HDF5","text":"HDF5 is used as the primary binary data format in LEGEND.","category":"page"},{"location":"hdf5/","page":"HDF5","title":"HDF5","text":"The following describes a mapping between the abstract data model and HDF5. This specifies the structure of the HDF5 implicitly, but precisely, for any data that conforms to the data model. The mapping purposefully uses only common and basic HDF5 features, to ensure it can be easily and reliably implemented in multiple programming languages.","category":"page"},{"location":"hdf5/#HDF5-datasets,-groups-and-attributes","page":"HDF5","title":"HDF5 datasets, groups and attributes","text":"","category":"section"},{"location":"hdf5/","page":"HDF5","title":"HDF5","text":"Different data types may be stored as an HDF5 dataset of the same type (e.g. a 2-dimensional dataset may represent a matrix or a vector of same-sized vectors). To make the HDF5 files self-documenting, the HDF5 attribute datatype is used to indicate the type semantics of datasets and groups.","category":"page"},{"location":"hdf5/#Basic-types","page":"HDF5","title":"Basic types","text":"","category":"section"},{"location":"hdf5/","page":"HDF5","title":"HDF5","text":"note: Quick reference\nData type datatype attribute\nScalar real, string, symbol, bool, ...\nFlat n-dimensional array array<n>{ELTYPE}\nFixed-sized n-dimensional array fixedsize_array<n>{ELTYPE}\nn-dimensional array of m-dimensional arrays of the same size array_of_equalsized_arrays<n,m>{ELTYPE}\nVector of vectors of different size array<1>{ARRAY-LIKE_DATATYPE}\nStruct struct{FIELDNAME_1,FIELDNAME_2,...}\nTable table{COLNAME_1,COLNAME_2,...}\nEnum enum{NAME_1=INT_VAL_1,NAME_2=INT_VAL_2,...}\nEncoded vector of vectors of different size array<1>{encoded_array<1>{ELTYPE}}\nEncoded array of arrays of the same size array_of_equalsized_encoded_arrays<n,m>{ELTYPE}","category":"page"},{"location":"hdf5/","page":"HDF5","title":"HDF5","text":"The abstract data model is mapped as follows:","category":"page"},{"location":"hdf5/#Scalar","page":"HDF5","title":"Scalar","text":"","category":"section"},{"location":"hdf5/","page":"HDF5","title":"HDF5","text":"Single scalar values are stored as 0-dimensional HDF5 datasets.","category":"page"},{"location":"hdf5/","page":"HDF5","title":"HDF5","text":"DATASET \"scalar\" {\n    ATTRIBUTE \"datatype\" = \"real\"\n    DATA = 3.14\n}","category":"page"},{"location":"hdf5/#Array","page":"HDF5","title":"Array","text":"","category":"section"},{"location":"hdf5/","page":"HDF5","title":"HDF5","text":"Collections of values. The datatype attribute will always contain array.","category":"page"},{"location":"hdf5/","page":"HDF5","title":"HDF5","text":"Flat n-dimensional arrays are stored as n-dimensional HDF5 datasets.","category":"page"},{"location":"hdf5/","page":"HDF5","title":"HDF5","text":"DATASET \"unixtime\" {\n    ATTRIBUTE \"datatype\" = \"array<1>{real}\"\n    ATTRIBUTE \"units\" = \"ns\"\n    DATA = [1.44061e+09, 1.44061e+09, ...]\n}","category":"page"},{"location":"hdf5/#Fixed-sized-array","page":"HDF5","title":"Fixed-sized array","text":"","category":"section"},{"location":"hdf5/","page":"HDF5","title":"HDF5","text":"warning: Warning\nUndocumented","category":"page"},{"location":"hdf5/#Array-of-equal-sized-arrays","page":"HDF5","title":"Array of equal-sized arrays","text":"","category":"section"},{"location":"hdf5/","page":"HDF5","title":"HDF5","text":"n-dimensional arrays of m-dimensional arrays of the same size are stored as flat n+m dimensional datasets.","category":"page"},{"location":"hdf5/","page":"HDF5","title":"HDF5","text":"DATASET \"waveform_values\" {\n    ATTRIBUTE \"datatype\" = \"array<1,1>{real}\"\n    DATA = [[13712, 13712, 13683, ..., 15400]\n            [13072, 13072, 12992, ..., 18806]\n            ...\n            [16918, 16918, 16962, ..., 18933]]\n}","category":"page"},{"location":"hdf5/#Vector-of-vectors","page":"HDF5","title":"Vector of vectors","text":"","category":"section"},{"location":"hdf5/","page":"HDF5","title":"HDF5","text":"A vector of vectors of unqual sizes is stored as an HDF5 group that contains two datasets:","category":"page"},{"location":"hdf5/","page":"HDF5","title":"HDF5","text":"An array-like dataset flattened_data that stores the concatenation of all vectors into a single vector. Can be *array<n>{...}, table{...}, etc.\nA 1-dimensional dataset cumulative_length that stores the cumulative sum of the length of all vectors (refers to axis 0 in flattened_data).","category":"page"},{"location":"hdf5/","page":"HDF5","title":"HDF5","text":"The two datasets in the group also have datatype (and possibly units) attributes that match their content.","category":"page"},{"location":"hdf5/","page":"HDF5","title":"HDF5","text":"GROUP \"vector_of_vectors\" {\n    ATTRIBUTE \"datatype\" = \"array<1>{ARRAY-LIKE_DATATYPE}\"\n    DATASET \"flattened_data\" {\n        ATTRIBUTE \"datatype\" = \"ARRAY-LIKE_DATATYPE\"\n        DATA = ...\n    }\n    DATASET \"cumulative_length\" {\n        ATTRIBUTE \"datatype\" = \"array<1>{real}\"\n        DATA = [3, 10, 34, ...]\n    }\n}","category":"page"},{"location":"hdf5/#Struct","page":"HDF5","title":"Struct","text":"","category":"section"},{"location":"hdf5/","page":"HDF5","title":"HDF5","text":"Structs are stored as HDF5 groups. Fields that are structs themselves are stored as sub-groups, scalars and arrays as datasets. Groups and datasets in the group are named after the fields of the struct.","category":"page"},{"location":"hdf5/","page":"HDF5","title":"HDF5","text":"GROUP \"struct\" {\n    ATTRIBUTE \"datatype\" = \"struct{array1,flag2,obj3}\n    DATASET \"array1\" {\n        ATTRIBUTE \"datatype\" = \"array<1>{real}\"\n        DATA = [5, 23, 4, ...]\n    }\n    DATASET \"flag2\" {\n        ATTRIBUTE \"datatype\" = \"bool\"\n        DATA = 0\n    }\n    GROUP \"obj3\" {\n        ATTRIBUTE \"datatype\" = \"struct{obj2}\"\n        ...\n    }\n}","category":"page"},{"location":"hdf5/#Table","page":"HDF5","title":"Table","text":"","category":"section"},{"location":"hdf5/","page":"HDF5","title":"HDF5","text":"A table is struct where all the fields (also called \"columns\") have the same length. It is stored as a group of datasets, each representing a column of the table.","category":"page"},{"location":"hdf5/","page":"HDF5","title":"HDF5","text":"GROUP \"waveform\" {\n    ATTRIBUTE \"datatype\" = \"table{t0,dt,values}\"\n    DATASET \"dt\" {\n        ATTRIBUTE \"datatype\" = \"array<1>{real}\"\n        ATTRIBUTE \"units\" = \"ns\"\n        DATA = [10, 10, 10, ...]\n    }\n    DATASET \"t0\" {\n        ATTRIBUTE \"datatype\"= \"array<1>{real}\"\n        ATTRIBUTE \"units\" = \"ns\"\n        DATA = [76420, 76420, 76420, ...]\n    }\n    GROUP \"values\" {\n        ATTRIBUTE \"datatype\"= \"array<1>{array<1>{real}}\"\n        DATASET \"cumulative_length\" {\n            ATTRIBUTE \"datatype\" = \"array<1>{real}\"\n            DATA = [1000, 2000, 3000, 4000, ...]\n        }\n        DATASET \"flattened_data\" {\n            ATTRIBUTE \"datatype\" = \"array<1>{real}\"\n            DATA = [14440, 14442, 14441, 14434, ...]\n        }\n    }\n}","category":"page"},{"location":"hdf5/#Enum","page":"HDF5","title":"Enum","text":"","category":"section"},{"location":"hdf5/","page":"HDF5","title":"HDF5","text":"Enum values are stored as integer values, but with the datatype attribute: enum{NAME=INT_VALUE,...}","category":"page"},{"location":"hdf5/","page":"HDF5","title":"HDF5","text":"DATASET \"evttype\" {\n    ATTRIBUTE \"datatype\" = \"array<1>{enum{evt_real=1,evt_pulser=2,evt_baseline=4}}\"\n    DATA = [1, 2, 1, 1, ...]\n}","category":"page"},{"location":"hdf5/#Values-with-physical-units","page":"HDF5","title":"Values with physical units","text":"","category":"section"},{"location":"hdf5/","page":"HDF5","title":"HDF5","text":"For values with physical units, the dataset only contains the numerical values. The attribute units stores the unit information. Its value is the string representation of the common scientific notation for the unit. Unicode must not be used.","category":"page"},{"location":"hdf5/","page":"HDF5","title":"HDF5","text":"DATASET \"energy\" {\n    ATTRIBUTE \"datatype\" = \"array<1>{real}\"\n    ATTRIBUTE \"units\" = \"keV\"\n    DATA = {2453.25, 234.34, 2039.22, ...]\n}","category":"page"},{"location":"hdf5/#Encoded-arrays","page":"HDF5","title":"Encoded arrays","text":"","category":"section"},{"location":"hdf5/","page":"HDF5","title":"HDF5","text":"Specialized structures should exist to represent encoded data. An important application is for lossless compression of waveform vectors (see Data Compression). All HDF5 objects representing encoded arrays must carry (in addition to the usual datatype) a codec string attribute holding the encoder algorithm identifier (a list can be found in Data Compression) in order to be decoded. Some decoders might require additional mandatory attributes.","category":"page"},{"location":"hdf5/#Encoded-[Array-of-equal-sized-arrays](@ref)","page":"HDF5","title":"Encoded Array of equal-sized arrays","text":"","category":"section"},{"location":"hdf5/","page":"HDF5","title":"HDF5","text":"An encoded array of equal-sized arrays is stored as an HDF5 group that contains two datasets:","category":"page"},{"location":"hdf5/","page":"HDF5","title":"HDF5","text":"encoded_data: the encoded data, for example a Vector of vectors. The type of the elements must be unsigned 8-bit integers (i.e. bytes)\ndecoded_size: 1-dimensional dataset that stores the lengths of the original (decoded) arrays","category":"page"},{"location":"hdf5/","page":"HDF5","title":"HDF5","text":"Example of encoded waveform values, where encoded_data is an array<1>{array<1>{real}} of bytes.","category":"page"},{"location":"hdf5/","page":"HDF5","title":"HDF5","text":"GROUP \"waveform_values\" {\n    ATTRIBUTE \"datatype\" = \"array_of_encoded_equalsized_arrays<1,1>{real}\"\n    ATTRIBUTE \"codec\" = \"radware_sigcompress\"\n    ATTRIBUTE \"codec_shift\" = -32768\n    GROUP \"encoded_data\" {\n        ATTRIBUTE \"datatype\" = \"array<1>{array<1>{real}}\"\n        DATASET \"cumulative_length\" {\n            ATTRIBUTE \"datatype\" = \"array<1>{real}\"\n            DATA = [...]\n        }\n        DATASET \"flattened_data\" {\n            ATTRIBUTE \"datatype\" = \"array<1>{real}\"\n            DATA = [...]\n        }\n    }\n    DATASET \"decoded_size\" {\n        ATTRIBUTE \"datatype\" = \"real\"\n        DATA = ...\n    }","category":"page"},{"location":"hdf5/#Encoded-[Vector-of-vectors](@ref)","page":"HDF5","title":"Encoded Vector of vectors","text":"","category":"section"},{"location":"hdf5/","page":"HDF5","title":"HDF5","text":"An encoded vector of vectors of unqual sizes is stored as an HDF5 group that contains two datasets:","category":"page"},{"location":"hdf5/","page":"HDF5","title":"HDF5","text":"encoded_data: the encoded data, for example a Vector of vectors. The type of the elements must be unsigned 8-bit integers (i.e. bytes)\ndecoded_size: 0-dimensional (i.e. scalar) dataset that stores the length of the original (decoded) arrays","category":"page"},{"location":"hdf5/#Histograms","page":"HDF5","title":"Histograms","text":"","category":"section"},{"location":"hdf5/","page":"HDF5","title":"HDF5","text":"A 1-dimensional histogram will be written as","category":"page"},{"location":"hdf5/","page":"HDF5","title":"HDF5","text":"GROUP \"hist_1d\" {\n    ATTRIBUTE \"datatype\" = \"struct{binning,weights,isdensity}\"\n    GROUP \"binning\" {\n        ATTRIBUTE \"datatype\" = \"struct{axis_1}\"\n        GROUP \"axis_1\" {\n            ATTRIBUTE \"datatype\" = \"struct{binedges,closedleft}\"\n            GROUP \"binedges\" {\n                ATTRIBUTE \"datatype\" = \"struct{first,last,step}\"\n                DATASET \"first\" {\n                    ATTRIBUTE \"datatype\" = \"real\"\n                    DATA = 0\n                }\n                DATASET \"last\" {\n                    ATTRIBUTE \"datatype\" = \"real\"\n                    DATA = 3000\n                }\n                DATASET \"step\" {\n                    ATTRIBUTE \"datatype\" = \"real\"\n                    DATA = 1\n                }\n            }\n            DATASET \"closedleft\" {\n                ATTRIBUTE \"datatype\" = \"bool\"\n                DATA = 1\n            }\n        }\n    }\n    DATASET \"isdensity\" {\n        ATTRIBUTE \"datatype\" = \"bool\"\n        DATA = 0\n    }\n    DATASET \"weights\" {\n        ATTRIBUTE \"datatype\" = \"array<1>{real}\"\n        DATA = [...]\n    }\n}","category":"page"},{"location":"hdf5/","page":"HDF5","title":"HDF5","text":"Multi-dimensional histograms will have groups axis_2, etc., with a multi-dimensional array as the value of dataset weights.","category":"page"},{"location":"data_compression/#Data-Compression","page":"Data Compression","title":"Data Compression","text":"","category":"section"},{"location":"data_compression/","page":"Data Compression","title":"Data Compression","text":"In addition to compression features provided by standard data formats (HDF5, etc.), LEGEND data uses some custom data compression.","category":"page"},{"location":"data_compression/","page":"Data Compression","title":"Data Compression","text":"In the interest of long-term data accessibility and to ensure compliance with FAIR data principles, use of custom data compression methods has to be limited to a minimum number of methods and use cases. Long-term use is only acceptable if:","category":"page"},{"location":"data_compression/","page":"Data Compression","title":"Data Compression","text":"The custom compression significantly outperforms standard compression methods in compression ratio and/or (de-)compression speed for important use cases.\nA complete formal description of the algorithms exists and is made publicly available under a license that allows for independent third-party implementations.\nVerified implementations exist in a least two different programming languages, at least one of which has been implemented independently from the formal description of the algorithm and at least one of which is made publicly available under an open-source license.","category":"page"},{"location":"data_compression/#Lossless-compression-of-integer-valued-waveform-vectors","page":"Data Compression","title":"Lossless compression of integer-valued waveform vectors","text":"","category":"section"},{"location":"data_compression/","page":"Data Compression","title":"Data Compression","text":"As detector waveforms have specific shapes, custom compression algorithms optimized for this use case can show a much higher speed/throughput than generic compression algorithms, at similar compression ratios.","category":"page"},{"location":"data_compression/","page":"Data Compression","title":"Data Compression","text":"Currently, we use the following custom integer-waveform compression algorithms:","category":"page"},{"location":"data_compression/","page":"Data Compression","title":"Data Compression","text":"Algorithm Identifier (codec attribute)\nradware-sigcompress v1.0 radware_sigcompress\nULEB128 ZigZag Differences uleb128_zigzag_diff","category":"page"},{"location":"data_compression/","page":"Data Compression","title":"Data Compression","text":"Other compression algorithms are being developed, tested and evaluated.","category":"page"},{"location":"data_compression/","page":"Data Compression","title":"Data Compression","text":"note: Note\nThe algorithm(s) in use are still subject to change, long-term data compatibility is not guaranteed at this point.","category":"page"},{"location":"data_compression/#ULEB128-ZigZag-Differences","page":"Data Compression","title":"ULEB128 ZigZag Differences","text":"","category":"section"},{"location":"data_compression/","page":"Data Compression","title":"Data Compression","text":"Efficient encoding of FADC waveform data can be achieved with the following algorithm:","category":"page"},{"location":"data_compression/","page":"Data Compression","title":"Data Compression","text":"Compute the waveform derivative (differences between adjacent samples). Prepend the result with the first value of the original waveform\nMap to positive values (including zero) via ZigZag encoding[1]\nCompute a binary variable-length representation of each value via Unsigned Little Endian Base-128 (7-bit) encoding[2].","category":"page"},{"location":"data_compression/","page":"Data Compression","title":"Data Compression","text":"[1]: https://wikipedia.org/wiki/Variable-length_quantity#Zigzag_encoding","category":"page"},{"location":"data_compression/","page":"Data Compression","title":"Data Compression","text":"[2]: https://wikipedia.org/wiki/LEB128#Unsigned_LEB128","category":"page"},{"location":"data_compression/#radware-sigcompress-v1.0","page":"Data Compression","title":"radware-sigcompress v1.0","text":"","category":"section"},{"location":"data_compression/","page":"Data Compression","title":"Data Compression","text":"There is no formal description of the radware-sigcompress algorithm yet, so the C-code of the original implementation (sigcompress.c) will serve as the reference for now:","category":"page"},{"location":"data_compression/","page":"Data Compression","title":"Data Compression","text":"// radware-sigcompress, v1.0\n//\n// This code is licensed under the MIT License (MIT).\n// Copyright (c) 2018, David C. Radford <radforddc@ornl.gov>\n\n#include \"stdio.h\"\n\nint compress_signal(short *sig_in, unsigned short *sig_out, int sig_len_in) {\n\n  int i, j, max1, max2, min1, min2, ds, nb1, nb2;\n  int iso, nw, bp, dd1, dd2;\n  unsigned short db[2];\n  unsigned int *dd = (unsigned int *)db;\n  static unsigned short mask[17] = {0,    1,    3,     7,     15,   31,\n                                    63,   127,  255,   511,   1023, 2047,\n                                    4095, 8191, 16383, 32767, 65535};\n\n  // static int len[17] = {4096, 2048,512,256,128, 128,128,128,128,\n  //                       128,128,128,128, 48,48,48,48};\n  /* ------------ do compression of signal ------------ */\n  j = iso = bp = 0;\n\n  sig_out[iso++] = sig_len_in; // signal length\n  while (j < sig_len_in) {     // j = starting index of section of signal\n    // find optimal method and length for compression of next section of signal\n    max1 = min1 = sig_in[j];\n    max2 = -16000;\n    min2 = 16000;\n    nb1 = nb2 = 2;\n    nw = 1;\n    for (i = j + 1; i < sig_len_in && i < j + 48;\n         i++) { // FIXME; # 48 could be tuned better?\n      if (max1 < sig_in[i])\n        max1 = sig_in[i];\n      if (min1 > sig_in[i])\n        min1 = sig_in[i];\n      ds = sig_in[i] - sig_in[i - 1];\n      if (max2 < ds)\n        max2 = ds;\n      if (min2 > ds)\n        min2 = ds;\n      nw++;\n    }\n    if (max1 - min1 <= max2 - min2) { // use absolute values\n      nb2 = 99;\n      while (max1 - min1 > mask[nb1])\n        nb1++;\n      // for (; i < sig_len_in && i < j+len[nb1]; i++) {\n      for (; i < sig_len_in && i < j + 128;\n           i++) { // FIXME; # 128 could be tuned better?\n        if (max1 < sig_in[i])\n          max1 = sig_in[i];\n        dd1 = max1 - min1;\n        if (min1 > sig_in[i])\n          dd1 = max1 - sig_in[i];\n        if (dd1 > mask[nb1])\n          break;\n        if (min1 > sig_in[i])\n          min1 = sig_in[i];\n        nw++;\n      }\n    } else { // use difference values\n      nb1 = 99;\n      while (max2 - min2 > mask[nb2])\n        nb2++;\n      // for (; i < sig_len_in && i < j+len[nb1]; i++) {\n      for (; i < sig_len_in && i < j + 128;\n           i++) { // FIXME; # 128 could be tuned better?\n        ds = sig_in[i] - sig_in[i - 1];\n        if (max2 < ds)\n          max2 = ds;\n        dd2 = max2 - min2;\n        if (min2 > ds)\n          dd2 = max2 - ds;\n        if (dd2 > mask[nb2])\n          break;\n        if (min2 > ds)\n          min2 = ds;\n        nw++;\n      }\n    }\n\n    if (bp > 0)\n      iso++;\n    /*  -----  do actual compression  -----  */\n    sig_out[iso++] = nw; // compressed signal data, first byte = # samples\n    bp = 0;              // bit pointer\n    if (nb1 <= nb2) {\n      /*  -----  encode absolute values  -----  */\n      sig_out[iso++] = nb1;                  // # bits used for encoding\n      sig_out[iso++] = (unsigned short)min1; // min value used for encoding\n      for (i = iso; i <= iso + nw * nb1 / 16; i++)\n        sig_out[i] = 0;\n      for (i = j; i < j + nw; i++) {\n        dd[0] = sig_in[i] - min1; // value to encode\n        dd[0] = dd[0] << (32 - bp - nb1);\n        sig_out[iso] |= db[1];\n        bp += nb1;\n        if (bp > 15) {\n          sig_out[++iso] = db[0];\n          bp -= 16;\n        }\n      }\n\n    } else {\n      /*  -----  encode derivative / difference values  -----  */\n      sig_out[iso++] = nb2 + 32; // # bits used for encoding, plus flag\n      sig_out[iso++] = (unsigned short)sig_in[j]; // starting signal value\n      sig_out[iso++] = (unsigned short)min2;      // min value used for encoding\n      for (i = iso; i <= iso + nw * nb2 / 16; i++)\n        sig_out[i] = 0;\n      for (i = j + 1; i < j + nw; i++) {\n        dd[0] = sig_in[i] - sig_in[i - 1] - min2; // value to encode\n        dd[0] = dd[0] << (32 - bp - nb2);\n        sig_out[iso] |= db[1];\n        bp += nb2;\n        if (bp > 15) {\n          sig_out[++iso] = db[0];\n          bp -= 16;\n        }\n      }\n    }\n    j += nw;\n  }\n\n  if (bp > 0)\n    iso++;\n  if (iso % 2)\n    iso++;    // make sure iso is even for 4-byte padding\n  return iso; // number of shorts in compressed signal data\n\n} /* compress_signal */\n\nint decompress_signal(unsigned short *sig_in, short *sig_out, int sig_len_in) {\n\n  int i, j, min, nb, isi, iso, nw, bp, siglen;\n  unsigned short db[2];\n  unsigned int *dd = (unsigned int *)db;\n  static unsigned short mask[17] = {0,    1,    3,     7,     15,   31,\n                                    63,   127,  255,   511,   1023, 2047,\n                                    4095, 8191, 16383, 32767, 65535};\n\n  /* ------------ do decompression of signal ------------ */\n  j = isi = iso = bp = 0;\n  siglen = (short)sig_in[isi++]; // signal length\n  // printf(\"<<< siglen = %d\\n\", siglen);\n  for (i = 0; i < 2048; i++)\n    sig_out[i] = 0;\n  while (isi < sig_len_in && iso < siglen) {\n    if (bp > 0)\n      isi++;\n    bp = 0;             // bit pointer\n    nw = sig_in[isi++]; // number of samples encoded in this chunk\n    nb = sig_in[isi++]; // number of bits used in compression\n\n    if (nb < 32) {\n      /*  -----  decode absolute values  -----  */\n      min = (short)sig_in[isi++]; // min value used for encoding\n      db[0] = sig_in[isi];\n      for (i = 0; i < nw && iso < siglen; i++) {\n        if (bp + nb > 15) {\n          bp -= 16;\n          db[1] = sig_in[isi++];\n          db[0] = sig_in[isi];\n          dd[0] = dd[0] << (bp + nb);\n        } else {\n          dd[0] = dd[0] << nb;\n        }\n        sig_out[iso++] = (db[1] & mask[nb]) + min;\n        bp += nb;\n      }\n\n    } else {\n      nb -= 32;\n      /*  -----  decode derivative / difference values  -----  */\n      sig_out[iso++] = (short)sig_in[isi++]; // starting signal value\n      min = (short)sig_in[isi++];            // min value used for encoding\n      db[0] = sig_in[isi];\n      for (i = 1; i < nw && iso < siglen; i++) {\n        if (bp + nb > 15) {\n          bp -= 16;\n          db[1] = sig_in[isi++];\n          db[0] = sig_in[isi];\n          dd[0] = dd[0] << (bp + nb);\n        } else {\n          dd[0] = dd[0] << nb;\n        }\n        sig_out[iso] = (db[1] & mask[nb]) + min + sig_out[iso - 1];\n        iso++;\n        bp += nb;\n      }\n    }\n    j += nw;\n  }\n\n  if (siglen != iso) {\n    printf(\"ERROR in decompress_signal: iso (%d ) != siglen (%d)!\\n\", iso,\n           siglen);\n  }\n  return siglen; // number of shorts in decompressed signal data\n\n} /* decompress_signal */","category":"page"},{"location":"#LEGEND-Data-Format-Specifications","page":"Home","title":"LEGEND Data Format Specifications","text":"","category":"section"},{"location":"#General-considerations","page":"Home","title":"General considerations","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"In the interest of long-term data accessibility and to ensure compliance with FAIR data principles,","category":"page"},{"location":"","page":"Home","title":"Home","text":"The number of different file formats should be kept to a reasonable minimum.\nOnly mature, well documented and and widely supported data formats with mature implementations/bindings for multiple programming languages are used.\nCustom file formats are, if at all, only used for raw data produced by DAQ systems. As raw data tends to be archived long-term, any custom raw data formats must fulfil the following requirements:\nA complete formal description of the format exists and is made publicly available under a license that allows for independent third-party implementations.\nAt least verified implementations is made publicly available under an open-source license.","category":"page"},{"location":"#Choice-of-file-formats","page":"Home","title":"Choice of file formats","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Depending on the kind of data, the following formats are preferred:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Binary data: HDF5\nMetadata: JSON","category":"page"},{"location":"#Abstract-data-model","page":"Home","title":"Abstract data model","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"LEGEND data should, wherever possible, be representable by a simple data model consisting of:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Scalar values\nVectors or higher-dimensional arrays. Arrays may be flat and contain scalar numerical values or nested and contain arrays, but must not contain structs or tables.\nStructs (resp. \"Dicts\" or named tuples) of named fields. Fields may contain scalar values, arrays or structs. In-memory representations of structs may be objects, named t\nTables (a.k.a. \"DataFrames\"), represented by structs of column-vectors of equal length.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Numerical values may be accompanied by physical units.","category":"page"},{"location":"","page":"Home","title":"Home","text":"A generic mapping of this data model must be defined for each file format used. The mapping must be self-documenting.","category":"page"},{"location":"metadata/#Metadata","page":"Metadata","title":"Metadata","text":"","category":"section"},{"location":"metadata/","page":"Metadata","title":"Metadata","text":"LEGEND metadata is stored in JSON. Formatting guidelines:","category":"page"},{"location":"metadata/","page":"Metadata","title":"Metadata","text":"In general, field names should be interpretable as valid variable names in common programming languages (e.g. use underscores (_) instead of dashes (-))","category":"page"},{"location":"metadata/#Physical-units","page":"Metadata","title":"Physical units","text":"","category":"section"},{"location":"metadata/","page":"Metadata","title":"Metadata","text":"Physical units should be specified as part of a field name by adding _in_<units> at the end. For example:","category":"page"},{"location":"metadata/","page":"Metadata","title":"Metadata","text":"{\n  \"radius_in_mm\": 11,\n  \"temperature_in_K\": 7\n}","category":"page"},{"location":"metadata/#Specifying-metadata-validity-in-time-(and-system)","page":"Metadata","title":"Specifying metadata validity in time (and system)","text":"","category":"section"},{"location":"metadata/","page":"Metadata","title":"Metadata","text":"LEGEND adopts a custom file format to specify the validity of metadata (for example a data production configuration that varies in time or according to the data taking mode), called JSONL (JSON + Legend).","category":"page"},{"location":"metadata/","page":"Metadata","title":"Metadata","text":"A JSONL file is essentially a collection of JSON-formatted records. Each record is formatted as follows:","category":"page"},{"location":"metadata/","page":"Metadata","title":"Metadata","text":"{\"valid_from\": \"TIMESTAMP\", \"category\": \"DATATYPE\", \"apply\": [\"FILE1\", \"FILE2\", ...]}","category":"page"},{"location":"metadata/","page":"Metadata","title":"Metadata","text":"where:","category":"page"},{"location":"metadata/","page":"Metadata","title":"Metadata","text":"TIMESTAMP is a LEGEND-style timestamp yyymmddThhmmssZ (in UTC time), also used to label data cycles, specifying the start of validity\nDATATYPE is the data type (all, phy, cal, lar, etc.) to which the metadata applies\napply takes an array of metadata files, to be combined \"in cascade\" (precedence order right to left) into the final metadata object","category":"page"},{"location":"metadata/","page":"Metadata","title":"Metadata","text":"The record above translates to:","category":"page"},{"location":"metadata/","page":"Metadata","title":"Metadata","text":"Combine FILE1, FILE2 etc. into a single metadata object. Fields in FILE2 override fields in FILE1. This metadata applies only to DATATYPE data and is valid from TIMESTAMP on.","category":"page"},{"location":"metadata/","page":"Metadata","title":"Metadata","text":"Records are stored in JSONL files one per line, without special delimiters:","category":"page"},{"location":"metadata/","page":"Metadata","title":"Metadata","text":"{\"valid_from\": \"TIMESTAMP1\", \"category\": \"DATATYPE1\", \"apply\": [\"FILE1\", \"FILE2\", ...]}\n{\"valid_from\": \"TIMESTAMP2\", \"category\": \"DATATYPE2\", \"apply\": [\"FILE3\", \"FILE4\", ...]}\n...","category":"page"}]
}
